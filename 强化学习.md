# 强化学习

## Markov Decision Process（MDP）有限Markov决策过程

- 系统状态集合为 $S$;
- 允许行为集合为 $A_{s} $,在状态 $s$ 下的行为集合$A_s$ ;
- 行动为 $a_{t}$ 下的状态转移概率 $P\left(\mathrm{~s}_{t+1} \mid s_{t}, a_{t}\right)=\sum_{r\in R}P(s_{t+1},r|s_t,a_t)$(动态特性)
- 得到即时回报（immediate reward） $r_{t+1}$ 的期望 为

$$
r\left(s_{t}, a_{t}, s_{t+1}\right)=E\left\{r_{t+1} \mid s_{t}, a_{t}, s_{t+1}\right)
$$

![](D:\blog\机器学习\figure\MDPoverview.png)

## Value Function

策略分为确定性策略和随机策略 

在某一个决策样本发生后我们可以得到他的回报函数:
$$
G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\cdots+\gamma^{r-1}R_r = \sum^{\infin}_{i=0}\gamma^iR_{t+i+1}(r\rightarrow\infin) \ \gamma\in[0,1]
$$
那么在决策过程中可以定义价值函数(Value Function)

$$
V_{\pi}(s) = E_{\pi}[G_t|S_t=s]\\
q_{\pi}(s, a) \triangleq E_{\pi}\left[G_{t}\mid S_{t}=s, A_{t}=a\right]
$$

- 在$V_{\pi}(s)$ 中 $\pi$ 为策略,定义了一个$s\rightarrow a$的随机映射

- $q_{\pi}(s, a)$ 中 $\pi$ 为策略,定义了一个与$s$ 无关的 $a$ 的决策

有以下三种的模型:

- 无限折扣模型 ( infinite-horizon discounted model)
  $V_{\pi}\left(s_{t}\right)=E\left\{r_{t+1}+\gamma_{i+2}+\gamma^{2} r_{r+3}+\cdots \mid s_{t}, \pi\right\}=E\left\{\sum_{k=0}^{\infty} \gamma^{k} r_{t+k+1} \mid s_{t}, \pi\right\}$

- 有限模型（finite-horizon model）
  $V_{\pi}\left(s_{t}\right)=E\left\{\sum_{k=0}^{h} r_{t+k+1} \mid s_{t}, \pi\right\}$

- 平均回报模型（average-reward model）

	 $V_{\pi}\left(s_{t}\right)=\frac{1}{h} E\left\{\sum_{k=0}^{h} r_{t+k+1} \mid s_{t}, \pi\right\} \quad$ 

## 贝尔曼期望方程 Bellman Exception Equation

两个价值函数有如下的关系

$$
V_{\pi}(s)=\sum_{a \in A} \pi(a \mid s) \cdot q_{\pi}(s, a)\\
	V_{\pi}(s)\leq \max(q_{\pi}(s, a))
$$
![](D:\blog\机器学习\figure\BellmanEeq.png)

## 贝尔曼最优方程

![image-20201204173612206](C:\Users\Michael Yu\AppData\Roaming\Typora\typora-user-images\image-20201204173612206.png) 

![image-20201204174403517](D:\blog\机器学习\image-20201204174403517.png)

## 动态规划

### 解析解

![image-20201205231010806](D:\blog\机器学习\image-20201205231010806.png)